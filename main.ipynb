{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== 开始训练场景: secondary_road ===\n",
      "从缓存文件加载数据: dataset/cache/secondary_road_30_1_dataset_pre_cache.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/100:   0%|          | 0/43 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from utils.dataset import ScenarioGraphDataset\n",
    "from torch_geometric.loader import DataLoader\n",
    "from utils.data_aug_model import Generator, Discriminator, train_gan, evaluate_generator\n",
    "from utils.dataset_utils import NODE_TYPE_MAP\n",
    "import torch.optim as optim\n",
    "\n",
    "import os\n",
    "os.environ[\"OMP_NUM_THREADS\"] = '1'\n",
    "\n",
    "# ------------------ 主程序示例 ------------------\n",
    "if __name__ == \"__main__\":\n",
    "    # 假设已有场景及数据路径\n",
    "    scene_datasets = {\n",
    "        \"secondary_road\": [\"dataset/driving-scene-graph/secondary-road\"],\n",
    "        \"ebike\": ['dataset/driving-scene-graph/ebike'],\n",
    "        \"main_secondary\": ['dataset/driving-scene-graph/main-secondary'],\n",
    "        \"motor\": [\n",
    "            \"dataset/driving-scene-graph/secondary-road\",\n",
    "            'dataset/driving-scene-graph/main-secondary'\n",
    "        ],\n",
    "        \"total\": [\n",
    "            \"dataset/driving-scene-graph/secondary-road\",\n",
    "            'dataset/driving-scene-graph/main-secondary',\n",
    "            'dataset/driving-scene-graph/ebike'\n",
    "        ]\n",
    "    }\n",
    "    # 数据集参数\n",
    "    window_size = 30  # 窗口步长\n",
    "    step_size = 1  # 假设的步长值\n",
    "    node_feature_dim = 19 + len(NODE_TYPE_MAP)  # 节点特征维度 # 假设改为10\n",
    "    num_classes = 3  # 标签类别数量\n",
    "\n",
    "    # 训练参数\n",
    "    hidden_dim = 64\n",
    "    num_epochs = 100\n",
    "    batch_size = 48\n",
    "    num_workers = 8  # 使用 4 个进程加载数据\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # 逐场景训练\n",
    "    for scene_name, root_dirs in scene_datasets.items():\n",
    "        print(f\"\\n=== 开始训练场景: {scene_name} ===\")\n",
    "\n",
    "        # 定义缓存路径，针对每个场景使用单独的缓存文件\n",
    "        cache_path = f\"dataset/cache/{scene_name}_{window_size}_{step_size}_dataset_pre_cache.pkl\" \n",
    "        generator_model_path = f\"model/data_aug/{scene_name}_{window_size}_{step_size}_generator_model.pth\"\n",
    "        checkpoint_path = f\"model/checkpoint/{scene_name}_generator_model_checkpoint.pth\" # 定义检查点路径\n",
    "\n",
    "        dataset = ScenarioGraphDataset(root_dirs, window_size, step_size, device, cache_path)\n",
    "        # 假设数据集的 80% 用于训练，20% 用于验证\n",
    "        train_size = int(0.8 * len(dataset))\n",
    "        val_size = len(dataset) - train_size\n",
    "        train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "\n",
    "        # 在初始化 DataLoader\n",
    "        train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "        val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "\n",
    "        # 初始化生成器和判别器, 并添加 Dropout\n",
    "        generator = Generator(node_feature_dim, hidden_dim, window_size, num_classes, dropout_rate=0.5).to(device)\n",
    "        discriminator = Discriminator(node_feature_dim, hidden_dim, window_size, dropout_rate=0.5).to(device)\n",
    "    \n",
    "        # 初始化优化器\n",
    "        g_optimizer = optim.AdamW(generator.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "        d_optimizer = optim.AdamW(discriminator.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "    \n",
    "        # 训练 GAN\n",
    "        train_gan(train_dataloader, generator, discriminator, g_optimizer, d_optimizer, num_epochs, device, window_size, val_dataloader, checkpoint_path, scene_name,num_classes=num_classes, generator_model_path=generator_model_path)\n",
    "\n",
    "        # 保存模型\n",
    "        # torch.save(generator.state_dict(), generator_model_path) # 已在早停中保存\n",
    "        # torch.save(discriminator.state_dict(), f\"{scene_name}_discriminator.pth\")\n",
    "        print(f\"场景 {scene_name} 训练完成\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import Batch\n",
    "from torch_geometric.loader import DataLoader\n",
    "from stnet.stnet import SpatioTemporalModel\n",
    "from utils.utils import ModelConfig, FocalLoss, train_model\n",
    "from utils.dataset import AugmentedScenarioGraphDataset\n",
    "from utils.dataset import ScenarioGraphDataset\n",
    "from utils.dataset_utils import NODE_TYPE_MAP, EDGE_TYPE_MAP\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # 设备配置\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.set_float32_matmul_precision('high')\n",
    "\n",
    "    # 滑动窗口配置\n",
    "    window_size = 30\n",
    "    step_size = 1\n",
    "\n",
    "    # 初始化配置\n",
    "    config = ModelConfig(\n",
    "        num_layers=3,\n",
    "        num_features=12 + len(NODE_TYPE_MAP),\n",
    "        hidden_dim=16,\n",
    "        num_relations=8,\n",
    "        edge_dim=8,\n",
    "        num_epochs=200,\n",
    "        num_classes=3,\n",
    "        window_size=window_size,\n",
    "        step_size=step_size\n",
    "    )\n",
    "\n",
    "    working_dir = \"./\"\n",
    "    model_dir = f\"{working_dir}/model\"\n",
    "    dataset_dir = \"./dataset\"\n",
    "\n",
    "    # 数据集配置\n",
    "    scene_datasets = {\n",
    "        \"main_secondary\": ['/main-secondary'],\n",
    "        \"secondary_road\": [\"/secondary-road\"],\n",
    "        \"motor\": [\n",
    "            \"/secondary-road\",\n",
    "            '/main-secondary'\n",
    "        ],\n",
    "        # \"ebike\": ['/ebike'],\n",
    "        # \"total\": [\n",
    "        #     \"/secondary-road\",\n",
    "        #     '/main-secondary',\n",
    "        #     '/ebike'\n",
    "        # ]\n",
    "    }\n",
    "\n",
    "    # 训练循环\n",
    "    for scene_name, data_dirs in scene_datasets.items():\n",
    "        print(f\"\\n=== Training Scene: {scene_name} ===\")\n",
    "        \n",
    "        data_dirs = [f\"{dataset_dir}\\driving-scene-graph{d}\" for d in data_dirs]\n",
    "        \n",
    "        # 数据集加载\n",
    "        cache_path = f\"{dataset_dir}/cache/{scene_name}_{window_size}_{step_size}_dataset_pre_cache.pkl\"\n",
    "        generator_model_path = f\"{working_dir}/model/data_aug/{scene_name}_{window_size}_{step_size}_generator_model.pth\"\n",
    "\n",
    "        # 加载数据集\n",
    "        dataset = ScenarioGraphDataset(data_dirs, window_size, step_size, device, cache_path)\n",
    "        weights = dataset.compute_class_weights()\n",
    "        \n",
    "        # 划分训练集和验证集\n",
    "        train_size = int(0.8 * len(dataset))\n",
    "        val_size = len(dataset) - train_size\n",
    "        train_dataset, val_dataset = torch.utils.data.random_split(dataset, \n",
    "                                                                    [train_size, val_size], \n",
    "                                                                    generator=torch.Generator().manual_seed(42))\n",
    "\n",
    "        # 创建数据加载器\n",
    "        train_loader = DataLoader(train_dataset, batch_size=512, shuffle=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=512, shuffle=False)\n",
    "\n",
    "        # 模型初始化\n",
    "        model = SpatioTemporalModel(config).to(device)\n",
    "        optimizer = optim.AdamW(model.parameters(), lr=1e-2, weight_decay=1e-4)\n",
    "        # criterion = FocalLoss(alpha=0.25, gamma=2)\n",
    "        criterion = nn.CrossEntropyLoss(weight=weights.to(device))\n",
    "        print(f\"class weights: {weights}\")\n",
    "\n",
    "        # 开始训练\n",
    "        train_model(\n",
    "            model=model,\n",
    "            train_loader=train_loader,\n",
    "            val_loader=train_loader,\n",
    "            optimizer=optimizer,\n",
    "            criterion=criterion,\n",
    "            device=device,\n",
    "            config=config,\n",
    "            checkpoint_dir=f\"model/checkpoint/\",\n",
    "            bestmodel_dir=f\"model/data_aug/\",\n",
    "            scene_name=scene_name,\n",
    "            patience=15\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Training Scene: secondary_road ===\n",
      "从缓存文件加载数据: ./dataset/cache/secondary_road_30_1_dataset_aug_cache.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/200: 100%|██████████| 16/16 [04:22<00:00, 16.41s/it, loss=0.0162]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Train Loss: 0.0156 | Val Acc: 0.3333 | Val Prec: 0.1113 | Val Recall: 0.3333 | Val F1: 0.1669\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/200: 100%|██████████| 16/16 [03:39<00:00, 13.73s/it, loss=0.0151]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 | Train Loss: 0.0150 | Val Acc: 0.3338 | Val Prec: 0.2843 | Val Recall: 0.3338 | Val F1: 0.1749\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/200: 100%|██████████| 16/16 [03:28<00:00, 13.05s/it, loss=0.0150]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 | Train Loss: 0.0150 | Val Acc: 0.3333 | Val Prec: 0.1176 | Val Recall: 0.3333 | Val F1: 0.1738\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/200:  62%|██████▎   | 10/16 [02:51<01:42, 17.12s/it, loss=0.0149]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Interrupt received, saving checkpoint...\n",
      "Checkpoint saved at epoch 3\n",
      "Best validation F1: 0.1749\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'exit' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 99\u001b[0m\n\u001b[0;32m     95\u001b[0m criterion \u001b[38;5;241m=\u001b[39m FocalLoss(alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.25\u001b[39m, gamma\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m     96\u001b[0m \u001b[38;5;66;03m# criterion = nn.CrossEntropyLoss()\u001b[39;00m\n\u001b[0;32m     97\u001b[0m \n\u001b[0;32m     98\u001b[0m \u001b[38;5;66;03m# 开始训练\u001b[39;00m\n\u001b[1;32m---> 99\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    100\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    101\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    102\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    103\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    104\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    105\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    106\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    107\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcheckpoint_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mmodel_dir\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/checkpoint\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    108\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbestmodel_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mmodel_dir\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/bestmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    109\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscene_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscene_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    110\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpatience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m15\u001b[39;49m\n\u001b[0;32m    111\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\OneDrive - chd.edu.cn\\Desktop\\毕业论文数据\\code\\STNet\\utils\\utils.py:177\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, train_loader, val_loader, optimizer, criterion, device, config, checkpoint_dir, bestmodel_dir, scene_name, patience)\u001b[0m\n\u001b[0;32m    173\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[0;32m    174\u001b[0m \u001b[38;5;66;03m# print(outputs)\u001b[39;00m\n\u001b[0;32m    175\u001b[0m \n\u001b[0;32m    176\u001b[0m \u001b[38;5;66;03m# 反向传播\u001b[39;00m\n\u001b[1;32m--> 177\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    178\u001b[0m nn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(model\u001b[38;5;241m.\u001b[39mparameters(), max_norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m30\u001b[39m, norm_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m    179\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32md:\\software\\anaconda3\\envs\\biye\\lib\\site-packages\\torch\\_tensor.py:626\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    616\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    617\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    618\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    619\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    624\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    625\u001b[0m     )\n\u001b[1;32m--> 626\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    627\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    628\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\software\\anaconda3\\envs\\biye\\lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\software\\anaconda3\\envs\\biye\\lib\\site-packages\\torch\\autograd\\graph.py:823\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    821\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    822\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 823\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    824\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    825\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    826\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    827\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "File \u001b[1;32md:\\OneDrive - chd.edu.cn\\Desktop\\毕业论文数据\\code\\STNet\\utils\\utils.py:124\u001b[0m, in \u001b[0;36mtrain_model.<locals>._signal_handler\u001b[1;34m(sig, frame)\u001b[0m\n\u001b[0;32m    122\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mInterrupt received, saving checkpoint...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    123\u001b[0m _save_checkpoint(epoch, force_save\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m--> 124\u001b[0m \u001b[43mexit\u001b[49m(\u001b[38;5;241m0\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'exit' is not defined"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m在当前单元格或上一个单元格中执行代码时 Kernel 崩溃。\n",
      "\u001b[1;31m请查看单元格中的代码，以确定故障的可能原因。\n",
      "\u001b[1;31m单击<a href='https://aka.ms/vscodeJupyterKernelCrash'>此处</a>了解详细信息。\n",
      "\u001b[1;31m有关更多详细信息，请查看 Jupyter <a href='command:jupyter.viewOutput'>log</a>。"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import wandb\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import Batch\n",
    "from torch_geometric.loader import DataLoader\n",
    "from stnet.stnet import SpatioTemporalModel\n",
    "from utils.utils import ModelConfig, FocalLoss, train_model\n",
    "from utils.dataset import AugmentedScenarioGraphDataset\n",
    "from utils.dataset_utils import NODE_TYPE_MAP, EDGE_TYPE_MAP\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # 设备配置\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.set_float32_matmul_precision('high')\n",
    "\n",
    "    # 滑动窗口配置\n",
    "    window_size = 30\n",
    "    step_size = 1\n",
    "\n",
    "    # 初始化配置\n",
    "    config = ModelConfig(\n",
    "        num_layers=3,\n",
    "        num_features=12 + len(NODE_TYPE_MAP),\n",
    "        hidden_dim=16,\n",
    "        num_relations=8,\n",
    "        edge_dim=8,\n",
    "        num_epochs=200,\n",
    "        num_classes=3,\n",
    "        window_size=window_size,\n",
    "        step_size=step_size\n",
    "    )\n",
    "\n",
    "    working_dir = \"./\"\n",
    "    model_dir = f\"{working_dir}/model\"\n",
    "    dataset_dir = \"./dataset\"\n",
    "\n",
    "    # 数据集配置\n",
    "    scene_datasets = {\n",
    "        \"motor\": [\n",
    "            \"/secondary-road\",\n",
    "            '/main-secondary'\n",
    "        ]\n",
    "    }\n",
    "    num_layer = 2\n",
    "    num_hidden = 16\n",
    "\n",
    "    # 训练循环\n",
    "    for scene_name, data_dirs in scene_datasets.items():\n",
    "        print(f\"\\n=== Training Scene: {scene_name} ===\")\n",
    "        \n",
    "        # 初始化 wandb\n",
    "        wandb.init(config={})  # 自动使用配置参数\n",
    "        config = wandb.config  # 获取超参数配置\n",
    "\n",
    "        # 数据集路径配置\n",
    "        data_dirs = [f\"{dataset_dir}/driving-scene-graph{d}\" for d in data_dirs]\n",
    "\n",
    "        # 数据集加载\n",
    "        cache_path = f\"{dataset_dir}/cache/{scene_name}_{config.window_size}_{config.step_size}_dataset_aug_cache.pkl\"\n",
    "        generator_model_path = f\"{working_dir}/model/data_aug/{scene_name}_{config.window_size}_{config.step_size}_{num_layer}_{num_hidden}_generator_model.pth\"\n",
    "\n",
    "        # 加载数据集\n",
    "        dataset = AugmentedScenarioGraphDataset(\n",
    "            root_dirs=data_dirs,\n",
    "            window_size=config.window_size,\n",
    "            step_size=config.step_size,\n",
    "            generator_model_path=generator_model_path,\n",
    "            node_feature_dim=config.num_features,\n",
    "            device=device,\n",
    "            cache_path=cache_path,\n",
    "            num_classes=config.num_classes\n",
    "        )\n",
    "\n",
    "        # 划分训练集和验证集\n",
    "        train_size = int(0.8 * len(dataset))\n",
    "        val_size = len(dataset) - train_size\n",
    "        train_dataset, val_dataset = torch.utils.data.random_split(dataset, \n",
    "                                                                    [train_size, val_size], \n",
    "                                                                    generator=torch.Generator().manual_seed(0))\n",
    "\n",
    "        # 创建数据加载器\n",
    "        train_loader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=config.batch_size, shuffle=False)\n",
    "\n",
    "        # 模型初始化\n",
    "        model = SpatioTemporalModel(config).to(device)\n",
    "        optimizer = optim.AdamW(model.parameters(), lr=config.learning_rate, weight_decay=config.weight_decay)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        # 开始训练\n",
    "        train_model(\n",
    "            model=model,\n",
    "            train_loader=train_loader,\n",
    "            val_loader=val_loader,\n",
    "            optimizer=optimizer,\n",
    "            criterion=criterion,\n",
    "            device=device,\n",
    "            config=config,\n",
    "            checkpoint_dir=f\"{model_dir}/checkpoint\",\n",
    "            bestmodel_dir=f\"{model_dir}/bestmodel\",\n",
    "            scene_name=scene_name,\n",
    "            patience=15\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: ov1u8zna\n",
      "Sweep URL: https://wandb.ai/1422909005/pytorch-sweeps-demo/sweeps/ov1u8zna\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: qjshsl7i with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfc_dropout: 0.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tgraph_dropout: 0.8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tgraph_num_head: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_dim: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlstm_bidirectional: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlstm_hidden_dim: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlstm_num_layers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_classes: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_layers: 4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_seed_points: 4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpool_ratio: 0.7\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tstep_size: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twindow_size: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "W&B syncing is set to <code>`offline`<code> in this directory.  <br>Run <code>`wandb online`<code> or set <code>WANDB_MODE=online<code> to enable cloud syncing."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'batch_size': 64, 'fc_dropout': 0.1, 'graph_dropout': 0.8, 'graph_num_head': 1, 'hidden_dim': 64, 'learning_rate': 0.01, 'lstm_bidirectional': False, 'lstm_hidden_dim': 32, 'lstm_num_layers': 2, 'num_classes': 3, 'num_layers': 4, 'num_seed_points': 4, 'pool_ratio': 0.7, 'step_size': 1, 'weight_decay': 0.01, 'window_size': 10}\n",
      "\n",
      "=== Training Scene: motor ===\n",
      "从缓存文件加载数据: ./dataset/cache/motor_10_1_dataset_aug_cache.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5:   2%|▏         | 2/84 [00:36<22:47, 16.68s/it, loss=1.1516]"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import wandb\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import Batch\n",
    "from torch_geometric.loader import DataLoader\n",
    "from stnet.stnet import SpatioTemporalModel\n",
    "from utils.utils import ModelConfig, FocalLoss, train_model\n",
    "from utils.dataset import AugmentedScenarioGraphDataset\n",
    "from utils.dataset_utils import NODE_TYPE_MAP, EDGE_TYPE_MAP\n",
    "\n",
    "\n",
    "def train(config=None):\n",
    "    # 设备配置\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.set_float32_matmul_precision('high')\n",
    "    with wandb.init(config=None, mode=\"offline\"):\n",
    "        configW = wandb.config\n",
    "        print(configW)  \n",
    "\n",
    "        # 初始化配置\n",
    "        config = ModelConfig(\n",
    "            num_layers=configW.num_layers,\n",
    "            num_features=12 + len(NODE_TYPE_MAP),\n",
    "            hidden_dim=configW.hidden_dim,\n",
    "            num_relations=8,\n",
    "            edge_dim=8,\n",
    "            num_epochs=5,\n",
    "            num_classes=configW.num_classes,\n",
    "            window_size=configW.window_size,\n",
    "            step_size=configW.step_size,\n",
    "            learning_rate=configW.learning_rate,\n",
    "            weight_decay=configW.weight_decay,\n",
    "            graph_num_head=configW.graph_num_head,\n",
    "            pool_ratio=configW.pool_ratio,\n",
    "            num_seed_points=configW.num_seed_points,\n",
    "            graph_dropout=configW.graph_dropout,\n",
    "            lstm_hidden_dim=configW.lstm_hidden_dim,\n",
    "            lstm_bidirectional=configW.lstm_bidirectional,\n",
    "            lstm_num_layers=configW.lstm_num_layers,\n",
    "            fc_dropout=configW.fc_dropout,\n",
    "            batch_size=configW.batch_size,\n",
    "        )\n",
    "\n",
    "        working_dir = \"./\"\n",
    "        model_dir = f\"{working_dir}/model\"\n",
    "        dataset_dir = \"./dataset\"\n",
    "\n",
    "        # 数据集配置\n",
    "        scene_datasets = {\n",
    "            \"motor\": [\n",
    "                \"/secondary-road\",\n",
    "                '/main-secondary'\n",
    "            ]\n",
    "        }\n",
    "        scene_name = \"motor\"\n",
    "        data_dirs = scene_datasets[scene_name]\n",
    "        \n",
    "        print(f\"\\n=== Training Scene: {scene_name} ===\")\n",
    "        \n",
    "        # 数据集路径配置\n",
    "        data_dirs = [f\"{dataset_dir}/driving-scene-graph{d}\" for d in data_dirs]\n",
    "\n",
    "        # 数据集加载\n",
    "        cache_path = f\"{dataset_dir}/cache/{scene_name}_{config.window_size}_{config.step_size}_dataset_aug_cache.pkl\"\n",
    "        generator_model_path = f\"{working_dir}/model/data_aug/{scene_name}_{30}_{1}_{2}_{16}_generator_model.pth\"\n",
    "\n",
    "        # 加载数据集\n",
    "        dataset = AugmentedScenarioGraphDataset(\n",
    "            root_dirs=data_dirs,\n",
    "            window_size=config.window_size,\n",
    "            step_size=config.step_size,\n",
    "            generator_model_path=generator_model_path,\n",
    "            node_feature_dim=config.num_features,\n",
    "            device=device,\n",
    "            cache_path=cache_path,\n",
    "            num_classes=config.num_classes\n",
    "        )\n",
    "\n",
    "        # 划分训练集和验证集\n",
    "        train_size = int(0.8 * len(dataset))\n",
    "        val_size = len(dataset) - train_size\n",
    "        train_dataset, val_dataset = torch.utils.data.random_split(dataset, \n",
    "                                                                    [train_size, val_size], \n",
    "                                                                    generator=torch.Generator().manual_seed(0))\n",
    "\n",
    "        # 创建数据加载器\n",
    "        train_loader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=config.batch_size, shuffle=False)\n",
    "\n",
    "        # 模型初始化\n",
    "        model = SpatioTemporalModel(config).to(device)\n",
    "        optimizer = optim.AdamW(model.parameters(), lr=config.learning_rate, weight_decay=config.weight_decay)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        # 开始训练\n",
    "        train_model(\n",
    "            model=model,\n",
    "            train_loader=train_loader,\n",
    "            val_loader=val_loader,\n",
    "            optimizer=optimizer,\n",
    "            criterion=criterion,\n",
    "            device=device,\n",
    "            config=config,\n",
    "            checkpoint_dir=f\"{model_dir}/checkpoint\",\n",
    "            bestmodel_dir=f\"{model_dir}/bestmodel\",\n",
    "            scene_name=scene_name,\n",
    "            patience=15\n",
    "        )\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    import yaml\n",
    "    with open(\"config/sweep.yaml\", \"r\", encoding=\"utf-8\") as file:\n",
    "        config_dict = yaml.safe_load(file)\n",
    "    sweep_id = wandb.sweep(config_dict, project=\"pytorch-sweeps-demo\")\n",
    "    wandb.agent(sweep_id, train, count=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

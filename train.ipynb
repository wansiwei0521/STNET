{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Training Scene: motor ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33m1422909005\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>d:\\OneDrive - chd.edu.cn\\Desktop\\毕业论文数据\\code\\STNet\\wandb\\run-20250215_235647-q02jezjr</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/1422909005/stnet-train-pygdata/runs/q02jezjr' target=\"_blank\">exquisite-heartthrob-12</a></strong> to <a href='https://wandb.ai/1422909005/stnet-train-pygdata' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/1422909005/stnet-train-pygdata' target=\"_blank\">https://wandb.ai/1422909005/stnet-train-pygdata</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/1422909005/stnet-train-pygdata/runs/q02jezjr' target=\"_blank\">https://wandb.ai/1422909005/stnet-train-pygdata/runs/q02jezjr</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: STGraphDataset(6427)\n",
      "Number of graphs: 6427\n",
      "Number of classes: 7\n",
      "每个类的数量: {2: 1895, 0: 1724, 1: 1887, 4: 412, 3: 164, 5: 259, 6: 86}\n",
      "class weights: tensor([ 0.5326,  0.4866,  0.4845,  5.5984,  2.2285,  3.5450, 10.6761])\n",
      "SpatioTemporalModel(\n",
      "  (graph_encoder): GraphEncoder(\n",
      "    (convs): ModuleList(\n",
      "      (0-2): 3 x RGATConv(256, 16, heads=1)\n",
      "    )\n",
      "    (norms): ModuleList(\n",
      "      (0-2): 3 x GraphNorm(256)\n",
      "    )\n",
      "    (feat_encoder): Linear(in_features=16, out_features=256, bias=True)\n",
      "    (feat_decoder): Linear(in_features=256, out_features=16, bias=True)\n",
      "    (norms_input): GraphNorm(256)\n",
      "    (pool): SAGPooling(GraphConv, 16, ratio=0.9, multiplier=1.0)\n",
      "    (aggr): SetTransformerAggregation(16, num_seed_points=4, heads=4, layer_norm=False, dropout=0)\n",
      "  )\n",
      "  (temporal_attn): TemporalAttention(\n",
      "    (query): Linear(in_features=64, out_features=64, bias=True)\n",
      "    (key): Linear(in_features=64, out_features=64, bias=True)\n",
      "    (value): Linear(in_features=64, out_features=64, bias=True)\n",
      "  )\n",
      "  (lstm): LSTM(64, 32, batch_first=True)\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=96, out_features=16, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.3, inplace=False)\n",
      "    (3): Linear(in_features=16, out_features=7, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/200: 100%|██████████| 322/322 [05:11<00:00,  1.03it/s, loss=3.8254]\n",
      "Epoch 2/200:  22%|██▏       | 71/322 [01:50<06:29,  1.55s/it, loss=3.5937]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training interrupted by user\n",
      "Best validation F1: 0.0604\n",
      "Checkpoint saved at epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>balanced_accuracy</td><td>▁</td></tr><tr><td>best_balanced_accuracy</td><td>▁</td></tr><tr><td>best_cohen_kappa</td><td>▁</td></tr><tr><td>best_matthews_corrcoef</td><td>▁</td></tr><tr><td>best_val_accuracy</td><td>▁</td></tr><tr><td>best_val_f1</td><td>▁</td></tr><tr><td>best_val_precision</td><td>▁</td></tr><tr><td>best_val_recall</td><td>▁</td></tr><tr><td>classifier_params</td><td>▁</td></tr><tr><td>cohen_kappa</td><td>▁</td></tr><tr><td>epoch</td><td>▁</td></tr><tr><td>graph_encoder_params</td><td>▁</td></tr><tr><td>lstm_params</td><td>▁</td></tr><tr><td>matthews_corrcoef</td><td>▁</td></tr><tr><td>temporal_attn_params</td><td>▁</td></tr><tr><td>train_loss</td><td>▁</td></tr><tr><td>trainable_params</td><td>▁</td></tr><tr><td>val_accuracy</td><td>▁</td></tr><tr><td>val_f1</td><td>▁</td></tr><tr><td>val_precision</td><td>▁</td></tr><tr><td>val_recall</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>balanced_accuracy</td><td>0.14286</td></tr><tr><td>best_balanced_accuracy</td><td>0.14286</td></tr><tr><td>best_cohen_kappa</td><td>0</td></tr><tr><td>best_matthews_corrcoef</td><td>0</td></tr><tr><td>best_val_accuracy</td><td>0.14286</td></tr><tr><td>best_val_f1</td><td>0.06044</td></tr><tr><td>best_val_precision</td><td>0.03832</td></tr><tr><td>best_val_recall</td><td>0.14286</td></tr><tr><td>classifier_params</td><td>1671</td></tr><tr><td>cohen_kappa</td><td>0</td></tr><tr><td>epoch</td><td>1</td></tr><tr><td>graph_encoder_params</td><td>32690</td></tr><tr><td>lstm_params</td><td>12544</td></tr><tr><td>matthews_corrcoef</td><td>0</td></tr><tr><td>temporal_attn_params</td><td>12480</td></tr><tr><td>train_loss</td><td>3.81005</td></tr><tr><td>trainable_params</td><td>59385</td></tr><tr><td>val_accuracy</td><td>0.14286</td></tr><tr><td>val_f1</td><td>0.06044</td></tr><tr><td>val_precision</td><td>0.03832</td></tr><tr><td>val_recall</td><td>0.14286</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">exquisite-heartthrob-12</strong> at: <a href='https://wandb.ai/1422909005/stnet-train-pygdata/runs/q02jezjr' target=\"_blank\">https://wandb.ai/1422909005/stnet-train-pygdata/runs/q02jezjr</a><br> View project at: <a href='https://wandb.ai/1422909005/stnet-train-pygdata' target=\"_blank\">https://wandb.ai/1422909005/stnet-train-pygdata</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250215_235647-q02jezjr\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# model training using pyg_dataset\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import wandb\n",
    "from torch_geometric.loader import DataLoader\n",
    "from stnet.stnet import SpatioTemporalModel\n",
    "from utils.utils import ModelConfig, FocalLoss, train_model\n",
    "from utils.pyg_dataset import STGraphDataset, MultiGraphData\n",
    "from utils.dataset_utils import NODE_TYPE_MAP, EDGE_TYPE_MAP\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # 设备配置\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.set_float32_matmul_precision('high')\n",
    "\n",
    "    # 滑动窗口配置\n",
    "    window_size = 30\n",
    "    step_size = 1\n",
    "    num_classes = 7\n",
    "\n",
    "    # 初始化配置\n",
    "    config = ModelConfig(\n",
    "        num_layers=3,\n",
    "        num_features=12 + len(NODE_TYPE_MAP),\n",
    "        hidden_dim=16,\n",
    "        num_relations=8,\n",
    "        edge_dim=8,\n",
    "        num_epochs=200,\n",
    "        num_classes=num_classes,\n",
    "        window_size=window_size,\n",
    "        step_size=step_size,\n",
    "        learning_rate=1e-3,\n",
    "        weight_decay=1e-4,\n",
    "        graph_num_head=4,\n",
    "        pool_ratio=0.9,\n",
    "        num_seed_points=4,\n",
    "        lstm_hidden_dim=32,\n",
    "        lstm_bidirectional=False,\n",
    "        lstm_num_layers=1,\n",
    "        fc_dropout=0.3,\n",
    "        batch_size=64,\n",
    "        gnn_query_dim=16,\n",
    "        gnn_num_head=1,\n",
    "        gnn_num_block=8\n",
    "    )\n",
    "\n",
    "    working_dir = \"./\"\n",
    "    model_dir = f\"{working_dir}/model\"\n",
    "\n",
    "    scene_name = \"motor\"\n",
    "    print(f\"\\n=== Training Scene: {scene_name} ===\")\n",
    "    # initialize wandb\n",
    "    wandb.init(project=\"stnet-train-pygdata\", config=config.__dict__)\n",
    "\n",
    "    # 加载数据集\n",
    "    work_dir = r'D:/OneDrive - chd.edu.cn/Desktop/毕业论文数据/code/STNet'\n",
    "    dataset = STGraphDataset(\n",
    "        root=os.path.join(work_dir, 'dataset', 'pyg_cache'),\n",
    "        pkl_file_path=os.path.join(work_dir, 'dataset', 'cache', 'motor_30_1_7_dataset_aug_cache.pkl')\n",
    "    )\n",
    "    print(f\"Dataset: {dataset}\")\n",
    "    print(f\"Number of graphs: {len(dataset)}\")\n",
    "    print(f\"Number of classes: {dataset.num_classes}\")\n",
    "    weights = dataset.compute_class_weights()\n",
    "    \n",
    "    # 划分训练集和验证集\n",
    "    train_size = int(0.8 * len(dataset))\n",
    "    val_size = len(dataset) - train_size\n",
    "    train_dataset, val_dataset = torch.utils.data.random_split(dataset, \n",
    "                                                                [train_size, val_size], \n",
    "                                                                generator=torch.Generator().manual_seed(0))\n",
    "\n",
    "    # 创建数据加载器\n",
    "    train_loader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=config.batch_size, shuffle=False)\n",
    "\n",
    "    # 模型初始化\n",
    "    model = SpatioTemporalModel(config).to(device)\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=config.learning_rate, weight_decay=config.weight_decay)\n",
    "    criterion = nn.CrossEntropyLoss(weight=weights.to(device))\n",
    "    print(f\"class weights: {weights}\")\n",
    "    \n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    wandb.log({\"trainable_params\": trainable_params})\n",
    "    for name, module in model.named_children():\n",
    "        params_count = sum(p.numel() for p in module.parameters() if p.requires_grad)\n",
    "        wandb.log({f\"{name}_params\": params_count})\n",
    "    print(model)\n",
    "\n",
    "    # 开始训练\n",
    "    train_model(\n",
    "        model=model,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        optimizer=optimizer,\n",
    "        criterion=criterion,\n",
    "        device=device,\n",
    "        config=config,\n",
    "        checkpoint_dir=f\"{model_dir}/checkpoint\",\n",
    "        bestmodel_dir=f\"{model_dir}/bestmodel\",\n",
    "        scene_name=scene_name,\n",
    "        patience=15\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\software\\anaconda3\\envs\\biye\\lib\\site-packages\\torch\\distributed\\distributed_c10d.py:750: UserWarning: Attempted to get default timeout for nccl backend, but NCCL support is not compiled\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "use_libuv was requested but PyTorch was build without libuv support",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 123\u001b[0m\n\u001b[0;32m    120\u001b[0m     dist\u001b[38;5;241m.\u001b[39mdestroy_process_group()\n\u001b[0;32m    122\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m--> 123\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[2], line 27\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     25\u001b[0m local_rank \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(os\u001b[38;5;241m.\u001b[39menviron\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLOCAL_RANK\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m0\u001b[39m))\n\u001b[0;32m     26\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mset_device(local_rank)\n\u001b[1;32m---> 27\u001b[0m \u001b[43mdist\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minit_process_group\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbackend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnccl\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minit_method\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43menv://\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     29\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m, local_rank)\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available():\n",
      "File \u001b[1;32md:\\software\\anaconda3\\envs\\biye\\lib\\site-packages\\torch\\distributed\\c10d_logger.py:81\u001b[0m, in \u001b[0;36m_exception_logger.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m     79\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs: _P\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: _P\u001b[38;5;241m.\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m _T:\n\u001b[0;32m     80\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 81\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     82\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m error:\n\u001b[0;32m     83\u001b[0m         msg_dict \u001b[38;5;241m=\u001b[39m _get_msg_dict(func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\software\\anaconda3\\envs\\biye\\lib\\site-packages\\torch\\distributed\\c10d_logger.py:95\u001b[0m, in \u001b[0;36m_time_logger.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs: _P\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: _P\u001b[38;5;241m.\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m _T:\n\u001b[0;32m     94\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m _WaitCounter(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpytorch.wait_counter.c10d.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mguard():\n\u001b[1;32m---> 95\u001b[0m         func_return \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func_return\n",
      "File \u001b[1;32md:\\software\\anaconda3\\envs\\biye\\lib\\site-packages\\torch\\distributed\\distributed_c10d.py:1714\u001b[0m, in \u001b[0;36minit_process_group\u001b[1;34m(backend, init_method, timeout, world_size, rank, store, group_name, pg_options, device_id)\u001b[0m\n\u001b[0;32m   1710\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m store \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1711\u001b[0m     rendezvous_iterator \u001b[38;5;241m=\u001b[39m rendezvous(\n\u001b[0;32m   1712\u001b[0m         not_none(init_method), rank, world_size, timeout\u001b[38;5;241m=\u001b[39mtimeout\n\u001b[0;32m   1713\u001b[0m     )\n\u001b[1;32m-> 1714\u001b[0m     store, rank, world_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mrendezvous_iterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1715\u001b[0m     store\u001b[38;5;241m.\u001b[39mset_timeout(timeout)\n\u001b[0;32m   1717\u001b[0m     \u001b[38;5;66;03m# Use a PrefixStore to avoid accidental overrides of keys used by\u001b[39;00m\n\u001b[0;32m   1718\u001b[0m     \u001b[38;5;66;03m# different systems (e.g. RPC) in case the store is multi-tenant.\u001b[39;00m\n",
      "File \u001b[1;32md:\\software\\anaconda3\\envs\\biye\\lib\\site-packages\\torch\\distributed\\rendezvous.py:274\u001b[0m, in \u001b[0;36m_env_rendezvous_handler\u001b[1;34m(url, timeout, **kwargs)\u001b[0m\n\u001b[0;32m    271\u001b[0m master_port \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(_get_env_or_raise(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMASTER_PORT\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m    272\u001b[0m use_libuv \u001b[38;5;241m=\u001b[39m _get_use_libuv_from_query_dict(query_dict)\n\u001b[1;32m--> 274\u001b[0m store \u001b[38;5;241m=\u001b[39m \u001b[43m_create_c10d_store\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    275\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmaster_addr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmaster_port\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrank\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mworld_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_libuv\u001b[49m\n\u001b[0;32m    276\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    278\u001b[0m \u001b[38;5;28;01myield\u001b[39;00m (store, rank, world_size)\n\u001b[0;32m    280\u001b[0m \u001b[38;5;66;03m# If this configuration is invalidated, there is nothing we can do about it\u001b[39;00m\n",
      "File \u001b[1;32md:\\software\\anaconda3\\envs\\biye\\lib\\site-packages\\torch\\distributed\\rendezvous.py:194\u001b[0m, in \u001b[0;36m_create_c10d_store\u001b[1;34m(hostname, port, rank, world_size, timeout, use_libuv)\u001b[0m\n\u001b[0;32m    192\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    193\u001b[0m     start_daemon \u001b[38;5;241m=\u001b[39m rank \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m--> 194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mTCPStore\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    195\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhost_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhostname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    196\u001b[0m \u001b[43m        \u001b[49m\u001b[43mport\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mport\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[43mworld_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mworld_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    198\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_master\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstart_daemon\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    199\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    200\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmulti_tenant\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    201\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_libuv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_libuv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: use_libuv was requested but PyTorch was build without libuv support"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.distributed as dist\n",
    "import torch.multiprocessing as mp\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torch_geometric.loader import DataLoader\n",
    "from stnet.stnet import SpatioTemporalModel\n",
    "from utils.utils import ModelConfig, FocalLoss, train_model\n",
    "from utils.pyg_dataset import STGraphDataset, MultiGraphData\n",
    "from utils.dataset_utils import NODE_TYPE_MAP, EDGE_TYPE_MAP\n",
    "\n",
    "def run(rank: int, world_size: int):\n",
    "    # 分布式初始化：根据环境变量获取 rank 和 world_size\n",
    "    os.environ['MASTER_ADDR'] = 'localhost'\n",
    "    os.environ['MASTER_PORT'] = '12345'\n",
    "    dist.init_process_group('nccl', rank=rank, world_size=world_size)\n",
    "    \n",
    "    device = torch.device(\"cuda\", rank)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.set_float32_matmul_precision('high')\n",
    "\n",
    "    # 滑动窗口配置及其他参数保持不变\n",
    "    window_size = 30\n",
    "    step_size = 1\n",
    "    num_classes = 7\n",
    "\n",
    "    config = ModelConfig(\n",
    "        num_layers=3,\n",
    "        num_features=12 + len(NODE_TYPE_MAP),\n",
    "        hidden_dim=16,\n",
    "        num_relations=8,\n",
    "        edge_dim=8,\n",
    "        num_epochs=200,\n",
    "        num_classes=num_classes,\n",
    "        window_size=window_size,\n",
    "        step_size=step_size,\n",
    "        learning_rate=0.001,\n",
    "        weight_decay=0.01,\n",
    "        graph_num_head=4,\n",
    "        pool_ratio=0.9,\n",
    "        num_seed_points=1,\n",
    "        lstm_hidden_dim=16,\n",
    "        lstm_bidirectional=False,\n",
    "        lstm_num_layers=1,\n",
    "        fc_dropout=0.3,\n",
    "        batch_size=16,\n",
    "        gnn_query_dim=16,\n",
    "        gnn_num_head=1\n",
    "    )\n",
    "\n",
    "    working_dir = \"./\"\n",
    "    model_dir = f\"{working_dir}/model\"\n",
    "    dataset_dir = \"/kaggle/input/pyg-scene-dataset\"\n",
    "\n",
    "    scene_name = \"motor\"\n",
    "    if rank == 0:\n",
    "        print(f\"\\n=== Training Scene: {scene_name} ===\")\n",
    "    \n",
    "    dataset = STGraphDataset(\n",
    "        root=os.path.join(working_dir, 'dataset', 'pyg_cache'),\n",
    "        pkl_file_path=os.path.join(dataset_dir, 'motor_30_1_7_dataset_aug_cache.pkl')\n",
    "    )\n",
    "    if rank == 0:\n",
    "        print(f\"Dataset: {dataset}\")\n",
    "        print(f\"Number of graphs: {len(dataset)}\")\n",
    "        print(f\"Number of classes: {dataset.num_classes}\")\n",
    "    weights = dataset.compute_class_weights()\n",
    "\n",
    "    # 划分训练集和验证集\n",
    "    train_size = int(0.8 * len(dataset))\n",
    "    val_size = len(dataset) - train_size\n",
    "    train_dataset, val_dataset = torch.utils.data.random_split(\n",
    "        dataset, [train_size, val_size], generator=torch.Generator().manual_seed(0)\n",
    "    )\n",
    "\n",
    "    # 使用 DistributedSampler 包装数据集（DDP 中 shuffle 应由 sampler 控制）\n",
    "    train_sampler = torch.utils.data.distributed.DistributedSampler(train_dataset)\n",
    "    val_sampler = torch.utils.data.distributed.DistributedSampler(val_dataset, shuffle=False)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=config.batch_size, sampler=train_sampler)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=config.batch_size, sampler=val_sampler)\n",
    "\n",
    "    # 初始化模型，并包装为 DDP 模型，启用未使用参数检测\n",
    "    model = SpatioTemporalModel(config).to(device)\n",
    "    model = DDP(model, device_ids=[rank], find_unused_parameters=True)\n",
    "    \n",
    "    optimizer = optim.AdamW(model.parameters(), lr=config.learning_rate, weight_decay=config.weight_decay)\n",
    "    criterion = nn.CrossEntropyLoss(weight=weights.to(device))\n",
    "    if rank == 0:\n",
    "        print(f\"class weights: {weights}\")\n",
    "\n",
    "    # 假设你使用 wandb 记录训练过程\n",
    "    if rank == 0:\n",
    "        import wandb\n",
    "        wandb.init(project=\"spatio-temporal\", config=config.__dict__)\n",
    "    \n",
    "    # 开始训练\n",
    "    train_model(\n",
    "        model=model,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        optimizer=optimizer,\n",
    "        criterion=criterion,\n",
    "        device=device,\n",
    "        config=config,\n",
    "        checkpoint_dir=f\"{model_dir}/checkpoint\",\n",
    "        bestmodel_dir=f\"{model_dir}/bestmodel\",\n",
    "        scene_name=scene_name,\n",
    "        patience=15\n",
    "    )\n",
    "    \n",
    "    # 结束分布式进程\n",
    "    dist.destroy_process_group()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    world_size = torch.cuda.device_count()\n",
    "    mp.spawn(run, args=(world_size,), nprocs=world_size, join=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
